[LORI-130]
Author: Dipl. Ing. Marvie Demit
---

# Activity 1.3: Conduct Initial AI System Inventory

**Phase:** 1 - Foundational Governance & Minimal Risk Compliance  
**Estimated Duration:** 2-4 weeks  
**Complexity:** Medium  
**Prerequisites:** Activity 1.2 (Inventory Procedure developed)

---

## 1. Activity Overview

### Purpose
Execute the inventory procedure developed in Activity 1.2 to identify and document all AI systems currently in use or development across the organization. This creates the foundational dataset for all subsequent compliance activities.

### Why This Matters
You cannot manage what you don't know exists. This inventory is legally required for High-Risk system registration (Article 71) and is the basis for risk classification, compliance planning, and resource allocation. Organizations consistently underestimate the number of AI systems they have - the average company discovers 2-3x more systems than initially expected.

### Key Deliverables
- **Completed AI System Inventory Register** ([LORI-131]) - Central database of all identified systems
- **Communication to Department Heads** ([LORI-132]) - Formal request for system identification
- **Inventory Completion Report** - Summary of findings and next steps

---

## 2. Implementation Tasks

### Week 1: Launch Inventory Campaign

**Step 1: Prepare Launch Materials**
- Customize communication template [LORI-132] with your organization's details
- Set clear deadline (typically 3-4 weeks from launch)
- Include link to identification form from Activity 1.2
- Prepare FAQ document addressing common questions

**Step 2: Conduct Department Head Briefing**
- Schedule 30-minute briefing session (live or recorded)
- Explain what qualifies as an AI system (use definition from Activity 1.2)
- Walk through the identification form
- Clarify expectations and deadline
- Designate point of contact for questions

**Step 3: Send Official Communication**
- Email from executive sponsor (not just compliance team)
- Include: purpose, deadline, form link, examples, contact info
- CC: AI Governance Body members for visibility

### Week 2-3: Collection and Follow-Up

**Step 4: Monitor Submission Progress**
- Track submissions by department (use spreadsheet or project management tool)
- Send reminder at Week 2 to non-responding departments
- Offer "office hours" for form completion assistance

**Step 5: Review Submitted Forms**
- Check completeness (all required fields filled)
- Verify AI system definition is correctly applied
- Flag unclear cases for follow-up
- Return incomplete forms with specific feedback

**Step 6: Conduct Targeted Outreach**
- Interview IT/procurement teams about vendor software with AI features
- Review recent software purchases for AI capabilities
- Check with data science/analytics teams for experimental systems
- Don't forget: chatbots, recommendation engines, predictive analytics, automated decision tools

### Week 3-4: Consolidation and Validation

**Step 7: Populate Inventory Register**
- Transfer approved submissions to [LORI-131] AI System Inventory Register
- Assign unique system IDs (e.g., AIS-001, AIS-002)
- Ensure all mandatory fields are complete
- Link to supporting documentation where available

**Step 8: Conduct Validation Review**
- Cross-check inventory against known systems
- Verify no obvious systems are missing (e.g., customer-facing chatbot)
- Spot-check with select departments for completeness
- Document any systems identified outside the formal process

**Step 9: Prepare Completion Report**
- Total systems identified (breakdown by department, type, vendor vs. internal)
- Comparison to initial estimates
- Preliminary risk distribution (if obvious)
- Identified gaps or challenges
- Recommendations for next steps

---

## 3. Implementation Checklist

### Launch Phase
- [ ] Communication template customized
- [ ] Department head briefing scheduled and conducted
- [ ] Official communication sent from executive sponsor
- [ ] FAQ document published
- [ ] Support contact designated and available

### Collection Phase
- [ ] Submission tracking system established
- [ ] Week 2 reminder sent
- [ ] All submitted forms reviewed for completeness
- [ ] Incomplete forms returned with feedback
- [ ] Targeted outreach conducted (IT, procurement, data science)

### Consolidation Phase
- [ ] All approved submissions entered in inventory register
- [ ] Unique system IDs assigned
- [ ] Validation review completed
- [ ] Completion report drafted
- [ ] AI Governance Body briefed on results

---

## 4. Common Pitfalls and Mitigation

### Pitfall 1: "We Don't Have Any AI Systems"
**Reality:** Most organizations have more AI than they think.

**Common Hidden AI:**
- CRM systems with predictive lead scoring
- Email platforms with spam filters or smart replies
- HR systems with resume screening
- Chatbots and virtual assistants
- Fraud detection in finance systems
- Recommendation engines in e-commerce
- Predictive maintenance in operations

**Mitigation:** Provide specific examples relevant to each department. Review vendor contracts for AI features.

---

### Pitfall 2: Low Response Rates
**Problem:** Departments don't respond or submit incomplete information.

**Mitigation:**
- Executive sponsor must send the request (not just compliance)
- Set clear deadline with consequences
- Make it easy (simple form, clear instructions, support available)
- Send multiple reminders
- Escalate non-compliance to department heads' managers
- Consider making inventory compliance part of performance reviews

---

### Pitfall 3: Vendor AI Not Reported
**Problem:** Employees don't realize vendor software contains AI.

**Example:** Salesforce Einstein, Microsoft 365 Copilot, Google Workspace AI features

**Mitigation:**
- Specifically ask about vendor/SaaS systems
- Review procurement records for the past 2 years
- Interview IT team about all software with "smart," "intelligent," "predictive," or "automated" features
- Check vendor documentation and contracts for AI capabilities

---

### Pitfall 4: Experimental/R&D Systems Excluded
**Problem:** Teams assume only production systems need to be reported.

**Reality:** EU AI Act applies to systems in development if they process real data or will be deployed in EU.

**Mitigation:**
- Explicitly state that R&D, pilot, and experimental systems must be reported
- Include deployment status field in identification form (Development/Testing/Production)
- Engage with innovation labs and data science teams directly

---

## 5. Practical Examples

### Example 1: Mid-Sized Bank
**Initial Estimate:** 5 AI systems  
**Actual Count:** 23 AI systems

**Breakdown:**
- 3 internally developed (fraud detection, credit scoring, chatbot)
- 20 vendor systems with AI features (CRM, email security, HR screening, transaction monitoring)

**Key Learning:** Vendor AI was 87% of total. Required procurement team involvement to identify.

---

### Example 2: Healthcare Provider
**Initial Estimate:** 2 AI systems  
**Actual Count:** 8 AI systems

**Breakdown:**
- 2 obvious (diagnostic support tools)
- 6 discovered through inventory (patient scheduling optimization, readmission prediction, medical coding assistance, clinical documentation AI, radiology workflow prioritization, staff rostering)

**Key Learning:** Clinical teams didn't recognize decision-support tools as "AI" until definition was clarified.

---

### Example 3: E-Commerce Retailer
**Initial Estimate:** 10 AI systems  
**Actual Count:** 12 AI systems

**Breakdown:**
- 8 vendor (product recommendations, search, chatbot, fraud detection, dynamic pricing, email personalization, inventory forecasting, customer segmentation)
- 4 internal (demand forecasting, warehouse optimization, customer lifetime value prediction, A/B test optimization)

**Key Learning:** Well-organized IT asset management made inventory straightforward. Completed in 2 weeks.

---

## 6. Related Templates

- **[LORI-131] AI System Inventory Register** - Central database template
- **[LORI-132] Communication Email to Department Heads** - Launch communication template
- **[LORI-121] AI System Identification Form** - From Activity 1.2

---

## 7. Success Metrics

**Immediate (End of Activity):**
- Response rate from departments: **Target >90%**
- Systems identified: **Target: All known systems captured**
- Inventory register completeness: **Target: 100% of required fields**

**Validation:**
- Spot-check validation: **Target: <5% additional systems found**
- AI Governance Body approval: **Yes/No**

---

## 8. Next Steps

Upon completion:
1. **Present findings to AI Governance Body** - Review inventory and approve register
2. **Proceed to Activity 1.4** - Develop Risk Classification Procedure
3. **Establish ongoing process** - New systems must be added to inventory before deployment

---

**Document Version:** 2.0  
**Last Updated:** December 2025